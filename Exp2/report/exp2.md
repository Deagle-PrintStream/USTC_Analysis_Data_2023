# 数据分析与实践实验二-实验报告

*刘眭怿 PB20061256 2023.3.19*

## 实验题目

**USTC 评课社区网站课程信息爬取**

## 实验要求

- 给定评课社区网站，需要设计一个网站遍历策略；
- 爬取至少200个课程的详细信息，包括字段如下表所示；
- 记录于 json 格式的文件中，并记录于csv格式的文件中。

| 序号 | 课程名称 | 课程难度 | 作业多少 | 给分好坏 | 收获大小 |
| -- | -- |-- |-- |-- |-- |
| 0 | 计算机程序设计A（白雪飞） |中等 |中等 |超好 |很多 

| 选课类别 | 教学类型 | 课程类型 | 开课单位 | 课程层次 | 学分 |
| -- |-- |-- |-- |-- |--|
|计划 |理论实验课 |本科计划内课程 |信息科学技术学院|通修 |4.0|

## 实现思路

- 首先解决课程信息网站url的遍历问题。`icourse.club`的主页是课程信息的列表，每页现实10个课程的信息，支持排序方式自定义。默认为课程类别：全部；排序方式：课程评分。由于不考虑爬取的200个课程的数据有特殊要求，故直接在该模式通过翻页的方式，得到200个课程详情页面的url。翻页的方式是通过改变page的数值实现：`https://icourse.club/course/?page=2`，因此设计一个循环结构，每次读取10条课程的url后翻页，直到收集满200条有效信息为止。
  ```
  while ( count < need):
    url="https://icourse.club/course/?page"+str(page_number)
    get_urls(url)
    count+=10
    page_number+=1
  ```

- 其次解决页面的读取问题。使用`urllib.request`实现对网页的访问，得到网页的源码。然后分析源码中关键信息的位置，利用正则表达式读取关键信息。其中，所有需要的课程信息均为静态文本，故可以直接读取，不需要利用`Selenium`等自动化测试的库。每读取一条课程的信息后，增加到一个总的列表类型中，作为目标数据库。这里需要明确字段名为：
  ```
  keys=["序号","课程名称","课程难度","作业多少","给分好坏","收获大小",\
        "选课类别","教学类型","课程类型","开课单位","课程层次","学分"]
  ```
- 最后解决数据保存的问题。为了保存为json格式的文件，引用`json`和`csv`的库，读取的信息保存的数据格式为`list[dict]`型，方便直接保存为json格式。对于csv格式，只需要字典的数据即可，不需要键值，需要额外处理一步，同时将键值保存为题头。
  ```
  json.dump(data_list,f_out)

  csvWriter=csv.writer(f_out)
  csvWriter.writerow(keys)
  csvWriter.writerows(csv_list)
  ```
## 实验总结

- 考虑到爬虫可能遇到未知的问题，陷入死循环。故设置一个最大翻页上限`page_number_max`。当爬虫读取到该上限的页面时，仍未收集到需要的有效课程信息，则强制停止脚本，保存数据并退出。
- `re`库不支持跨行匹配，故需要将网页源码中的换行符均替换掉，即整合成单行的字符串型。
- 考虑到200条数据的容量不算大，故在爬取过程中没有设置备份等操作。
- "课程难度"等关键字在网页源码中多次出现，原因是每条学生的评价都会复制一份同样的字段数据，因此需要先找到该课程的综合评价所在的网页框架位置。
- 由于数据的内容含有中文字符，故在保存的时候需要指定非强制ASCII型，同时为了美观，设置了缩进形式，即
  `json.dump(data_list,f_out,indent=2,ensure_ascii=False)`
- 鉴于非企业的专业服务器，故没有考虑对于反爬虫的应对措施，只是设置在每爬取10条记录后，随机休眠2秒左右。同时，在本机上利用校园网对`icourse.club`进行访问，每次的握手等待时间约为0.5秒左右。

